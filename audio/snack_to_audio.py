from snac import SNAC
import numpy as np
import torch
import soundfile as sf
#############################################
def reconscruct_snac(output_list):
    output = []
    for i in range(len(output_list[-1])):
        output.append("#")
        for j in range(7):
            output.append(output_list[j][i])
    return output


def reconstruct_tensors(flattened_output, device=None):
    """Reconstructs the list of tensors from the flattened output."""

    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def count_elements_between_hashes(lst):
        try:
            # Find the index of the first '#'
            first_index = lst.index("#")
            # Find the index of the second '#' after the first
            second_index = lst.index("#", first_index + 1)
            # Count the elements between the two indices
            return second_index - first_index - 1
        except ValueError:
            # Handle the case where there aren't enough '#' symbols
            return "List does not contain two '#' symbols"

    def remove_elements_before_hash(flattened_list):
        try:
            # Find the index of the first '#'
            first_hash_index = flattened_list.index("#")
            # Return the list starting from the first '#'
            return flattened_list[first_hash_index:]
        except ValueError:
            # Handle the case where there is no '#'
            return "List does not contain the symbol '#'"

    def list_to_torch_tensor(tensor1):
        # Convert the list to a torch tensor
        tensor = torch.tensor(tensor1)
        # Reshape the tensor to have size (1, n)
        tensor = tensor.unsqueeze(0)
        return tensor

    flattened_output = remove_elements_before_hash(flattened_output)
    codes = []
    tensor1 = []
    tensor2 = []
    tensor3 = []
    tensor4 = []

    n_tensors = count_elements_between_hashes(flattened_output)
    if n_tensors == 7:
        for i in range(0, len(flattened_output), 8):

            tensor1.append(flattened_output[i + 1])
            tensor2.append(flattened_output[i + 2])
            tensor3.append(flattened_output[i + 3])
            tensor3.append(flattened_output[i + 4])

            tensor2.append(flattened_output[i + 5])
            tensor3.append(flattened_output[i + 6])
            tensor3.append(flattened_output[i + 7])
            codes = [
                list_to_torch_tensor(tensor1).to(device),
                list_to_torch_tensor(tensor2).to(device),
                list_to_torch_tensor(tensor3).to(device),
            ]

    if n_tensors == 15:
        for i in range(0, len(flattened_output), 16):

            tensor1.append(flattened_output[i + 1])
            tensor2.append(flattened_output[i + 2])
            tensor3.append(flattened_output[i + 3])
            tensor4.append(flattened_output[i + 4])
            tensor4.append(flattened_output[i + 5])
            tensor3.append(flattened_output[i + 6])
            tensor4.append(flattened_output[i + 7])
            tensor4.append(flattened_output[i + 8])

            tensor2.append(flattened_output[i + 9])
            tensor3.append(flattened_output[i + 10])
            tensor4.append(flattened_output[i + 11])
            tensor4.append(flattened_output[i + 12])
            tensor3.append(flattened_output[i + 13])
            tensor4.append(flattened_output[i + 14])
            tensor4.append(flattened_output[i + 15])

            codes = [
                list_to_torch_tensor(tensor1).to(device),
                list_to_torch_tensor(tensor2).to(device),
                list_to_torch_tensor(tensor3).to(device),
                list_to_torch_tensor(tensor4).to(device),
            ]

    return codes
#####################################################################


snacmodel = SNAC.from_pretrained("hubertsiuzdak/snac_24khz").eval()

def load_snac_token(x: str):
    # 按 # 分割成多行
    lines = x.split('#')

    # 提取每行的整数并转换为 np.int16
    int_arrays = []
    for line in lines:
        if line.strip():  # 跳过空行
            int_list = list(map(int, line.strip().split()))
            int_arrays.append(np.array(int_list, dtype=np.int16))

    # 将所有整数连接成一个 np.array
    result_array = np.stack(int_arrays)
    return result_array


codec_label = "# 1942 2031 2825 2111 2743 271 3545 # 868 3867 50 2736 3070 806 1462 # 2534 3226 2776 1771 3270 4078 606 # 1077 3697 1748 1465 2180 2775 2256 # 578 3154 453 2741 3689 837 2853 # 1162 936 3672 876 3251 1038 2795 # 651 4022 1308 3776 2039 1859 2808 # 803 1245 2522 119 1241 1704 215 # 1942 2735 180 3128 3379 1313 3713 # 2742 1265 3086 3262 2380 3573 3169 # 3395 1306 406 2793 2039 767 392 # 2535 2981 2126 3484 2118 857 3927 # 1682 400 1775 689 3761 3000 1662 # 1680 2998 323 3316 580 2994 3549 # 2345 976 815 2072 3889 2287 2473 # 2985 3352 473 3776 2481 3623 2091 # 1769 2284 4019 758 1870 3267 3578 # 2631 592 2384 546 3397 3994 530 # 246 3397 2651 2424 978 3867 1839 # 1942 2377 3269 602 3114 1961 2066 # 2434 3418 2462 3613 1410 1362 431 # 1635 2464 925 4000 2464 2506 3342 # 1636 1630 1550 3704 1410 2588 3579 # 3634 3620 2733 1827 475 172 2056 # 2985 1558 414 3193 555 514 1521 # 1841 1346 1749 1043 3036 3326 2680 # 3310 2256 3523 1514 1583 196 836 # 1288 1139 1115 1379 3523 2870 450 # 3395 2056 362 3141 1630 1173 3668 # 2414 2151 3629 864 3032 196 1465 # 372 3323 3088 4067 1742 4094 1464 # 3121 2762 2193 2142 3352 648 92 # 2686 2403 481 1376 3790 2204 974 # 1467 518 3629 1775 993 1048 2952 # 1291 1558 3962 1581 3446 1940 2946 # 306 1284 3025 3621 922 3352 3733 # 1467 410 2681 2256 1048 1636 1631 # 2816 127 307 103 3846 3596 2193 # 3982 3790 1561 969 127 2690 715 # 3330 766 128 350 557 1204 2809 # 1794 779 1455 3945 1442 2932 336 # 1523 4009 3502 1976 1039 159 3148 # 1991 1956 885 2757 3062 3599 1044 # 1596 1238 3934 2515 2482 1571 3056 # 2926 1946 3559 544 1530 1633 1859 # 1135 2897 3801 3801 1666 3605 2010 # 3982 3073 1746 2273 3251 2044 162 # 2352 3858 706 890 1265 388 1564 # 2515 3004 2047 344 2939 403 209 # 1549 1033 688 3736 3064 1184 3080 # 99 2259 790 3653 3388 1277 3460 # 1415 1011 3003 1972 3450 4036 2471 # 1210 1900 725 933 580 640 1972 # 3073 1442 3491 21 1558 1142 1541 # 2225 3882 602 2385 431 921 397 # 957 3446 456 1533 766 3971 1163 # 2104 1473 3826 1523 1018 1704 1755 # 3553 218 1085 1715 2505 454 1744 # 3395 1317 974 2634 353 3425 420 # 3069 2384 3846 2733 1003 3579 3037 # 588 264 2260 601 940 3924 2133 # 780 3368 420 1474 1265 1279 4065 # 3411 1852 653 590 3171 1344 659 # 1354 3568 3515 3330 1983 2067 2103 # 1467 475 407 403 1672 531 557 # 2310 665 2884 1257 193 1367 1510 # 555 826 1466 62 1943 756 998 # 1488 1201 1829 115 1210 1515 3706 # 19 1943 3133 1179 1303 2722 544 # 2898 2323 387 1023 3761 3579 1623 # 3982 2617 1895 1521 1114 2069 180 # 2916 1139 1582 2755 1599 4058 2601 # 1415 2838 2736 1896 3552 1606 3939 # 3395 3130 400 676 779 3843 3019 # 1349 766 1207 1698 2257 2865 3330 # 3136 1012 646 540 2518 2876 3293 # 3195 3882 733 3970 3842 3850 1611 # 3230 1891 3712 1674 462 4067 1212 # 1484 3380 1536 81 1891 1633 3069 # 2548 2252 955 2029 2112 929 3682 # 2355 3380 42 3262 2573 1235 1123 # 563 935 3269 3916 3378 2178 1083 # 2719 2789 1505 728 1946 3143 2308 # 563 525 1172 4007 1300 910 3740 # 531 1303 42 1960 1442 2348 108 # 2816 2403 2713 1322 756 2700 2334 # 2468 165 3668 2949 800 1211 3491 # 3803 2053 2658 2928 1803 2475 1732 # 1488 3697 3284 31 2267 2288 3562 # 2760 87 3394 3570 165 2222 1157 # 949 165 2192 42 1442 998 3768 # 1488 652 221 2555 3620 2038 1469 # 3272 492 2695 2114 1148 2612 1367 # 2914 1424 3996 1981 468 1424 417 # 3309 391 2667 2240 3392 3613 3704 # 2418 1742 3345 1080 787 2241 1984 # 3566 3129 381 717 520 3466 3285 # 2049 1992 3976 3976 165 583 1819 # 504 2308 57 2055 2794 923 1198 # 1919 3618 3554 2721 2725 1356 695 # 296 2727 3481 3986 2068 3523 2131 # 1467 493 393 1462 3500 1566 1686 # 780 1748 163 1675 3687 3564 2882 # 1096 1418 1815 1036 437 335 336 # 9 3554 2763 3871 3524 2475 819 # 3453 161 1278 3502 602 42 3740 # 30 2119 3634 472 840 1534 208 # 2202 3579 2927 3782 3715 1183 1933 # 906 1012 3028 3460 3001 1512 590 # 411 1255 3012 1221 2832 3021 3913 # 1572 1166 140 3161 468 2778 535 # 2277 751 3494 3605 2641 1647 3342 # 1537 2384 3704 1501 3130 2615 2821 # 2573 3130 303 54 1487 3684 1122 # 1218 3546 742 2255 3233 1855 1791 # 436 1128 1860 1641 3231 513 1240 # 74 2347 811 218 1742 3218 4058 # 3780 2585 2953 1219 2505 2222 788 # 1387 1558 3117 780 2323 376 1651 # 3168 1210 1176 3992 1742 2648 3547 # 1785 2151 2054 2193 174 4026 916 # 2493 2050 2817 334 1399 1300 349 # 2135 1416 878 3597 358 2129 741 # 2049 311 3698 1887 2855 2578 706 # 1185 602 3758 3226 2104 2056 3054 # 813 3620 3749 419 3380 637 2350 # 3754 3036 1225 1220 2083 3124 3634 # 153 3413 3426 2852 28 2746 3001 # 1354 1900 2772 2810 3620 758 2794 # 2717 410 949 2841 892 1712 334 # 1019 1824 3781 3693 2267 456 847 # 1136 2883 2425 3844 3286 587 3760 # 3738 3165 458 2721 2503 799 3393 # 3403 164 885 2458 2615 1321 311 # 2939 3892 1922 4080 396 2697 2370 # 3870 1223 3520 3851 1218 2883 1808 # 1467 997 3486 1232 2214 4058 3986 # 3403 137 1572 1129 2557 2339 1172 # 1488 3380 3862 140 1852 2073 2515 # 153 1283 3818 1496 79 3794 500 # 3369 2951 1302 484 1742 403 3619 # 2434 518 1850 1889 1197 3284 551 # 1361 766 1957 4067 1442 2113 1298 # 1769 3620 1331 760 1696 1081 3577 # 3780 169 3143 3528 707 121 3572 # 4066 2106 2725 1671 37 271 2706 # 3265 3500 2303 298 1019 104 3436 # 2931 3037 2517 3485 1241 3579 1798 # 3101 1018 1092 2557 98 1617 1062 # 3634 1033 3697 733 2765 1358 4027 # 3982 2205 3186 1343 1666 1675 2470 # 2107 883 2738 418 1200 424 1587 # 213 1552 961 1012 1317 2551 888 # 3218 983 987 874 3974 3706 4025 # 1162 560 3569 3191 281 2484 2095 # 3395 3844 1600 2362 1244 2507 389 # 2277 4016 1006 430 1402 640 2518 # 3311 3647 472 3533 3790 1464 2016 # 2686 518 2016 158 948 2873 1358 # 2179 120 896 291 2585 1543 1865 # 1467 1456 3161 1385 2123 2455 2241 # 1387 2000 2075 943 706 3337 491 # 2345 2798 3766 162 3733 1769 3309 # 2650 3406 1024 1290 305 1917 2914 # 3241 2930 2174 1695 28 566 1788 # 2535 1858 609 3290 1128 2741 3850 # 3403 3380 2290 3717 2323 3607 1118 # 1228 1011 2057 160 2151 2561 1240 # 1467 1600 2131 3218 3500 2837 626 # 564 3867 1509 40 1291 2225 1840 # 3228 3103 57 750 3368 3763 1979 # 3053 1065 1769 625 3299 1940 3240 # 2382 305 298 2562 28 636 863 # 3395 1558 3363 22 3380 928 1170 # 1769 1210 2146 201 3646 214 3829 # 1133 1742 818 218 1700 2109 2354 # 3950 1961 2458 3890 863 1117 1449 # 3506 4077 1012 2215 1909 4019 1460 # 3168 2798 2676 1662 160 949 3165 # 436 353 362 3657 3380 2810 167 # 667 3034 262 1982 3620 2932 1474 # 1387 28 1591 715 1900 356 168 # 2433 1755 85 2476 3592 2229 335 # 3395 1819 3082 1695 3013 3166 915 # 1349 779 1373 519 2112 2409 3799 # 2816 4009 3226 1496 2050 2561 3554 # 296 1456 3418 2533 16 2029 1721 # 2384 173 1739 2759 2323 218 3679 # 2049 3251 2642 1309 3882 3864 2538 # 3869 3962 3406 3465 1424 2437 3304 # 2566 1518 794 2949 809 3329 587 # 3651 4065 3196 1567 3523 3552 2150 # 3869 2335 1883 3671 4058 3363 2380 # 588 3290 2579 3808 2961 230 3186 # 780 2065 2566 2477 244 1783 1891 # 803 2172 158 1237 627 3435 172 # 1071 1829 229 2056 94 681 1222 # 1484 2257 3460 720 2897 1191 4088 # 1964 442 2272 3424 2505 1381 484 # 1195 3573 1865 648 1139 459 2503 # 1185 200 21 3770 3608 2773 2565 # 3019 3130 1855 810 555 2897 2090 # 2277 1873 771 2830 3552 3316 723 # 2036 1718 3678 711 2754 3559 1304 # 3381 2585 2070 2051 3811 3358 1086 # 2414 2353 2987 3986 3905 148 2203 # 1354 3956 4066 1429 1033 78 2899 # 499 3034 4013 3490 1255 1245 1629 # 3395 1229 1121 3652 284 2256 319 # 3281 1453 1268 3394 1819 2741 1959 # 74 2671 3431 720 590 3730 3579 # 3985 3773 3293 696 2833 1405 162 # 153 1048 3579 2057 955 1628 1648 # 153 3301 1115 1860 4039 389 983 # 3084 1956 927 838 2935 4083 969 # 1241 1886 3267 2772 1175 97 429 # 2105 1095 3520 4083 879 1156 1631 # 3069 757 617 1887 3790 4091 744 # 751 2747 3087 564 3162 1808 2812 # 3151 200 978 3174 1303 2445 1862 # 3453 462 1172 2374 1658 3761 1173 # 3991 668 4039 197 3046 2012 290 # 2554 958 1994 2269 861 307 2981 # 2456 608 2689 120 2064 1393 2018 # 2355 3380 3647 3435 3013 463 790 # 1135 1285 134 1909 3130 356 2793 # 1135 323 2455 3970 127 1611 782 # 1245 2617 2113 3992 1139 3088 3196 # 2352 1290 1305 1942 2563 1104 3061 # 1705 2425 3217 3437 1948 1054 2155 # 2961 1390 362 2586 2642 606 3155 # 296 2409 2561 1304 1241 2213 3990 # 1899 1478 389 2055 602 327 172 # 2909 1437 1173 2938 1801 1173 1492 # 2132 567 479 605 1886 1142 160 # 372 2151 2203 3669 2296 1714 1194 # 3073 1442 3077 678 1514 2961 2894 # 2310 1185 3994 3591 2554 3944 3747 # 3803 2226 2547 134 2585 856 3466 # 1942 1381 3112 2983 1086 4067 3048 # 2833 2031 2917 283 1312 3901 3506 # 1361 200 695 3625 2464 2140 2838 # 1603 1983 3351 2890 2231 3295 3663 # 1596 525 651 1372 2101 3306 3807 # 2392 1097 1895 2016 2112 756 1659 # 813 284 3976 471 1197 3631 2090 # 3944 1255 558 640 3171 1133 2305 # 2037 434 129 3557 1742 2224 3085 # 3154 3834 2142 2888 2347 2454 922 # 471 2677 2948 868 505 1922 3216 # 2017 2971 1392 1164 2677 3460 3037 # 327 1442 3042 3077 3882 3033 1317 # 2310 165 224 2300 1900 844 358 # 3672 2115 1833 1246 1381 1529 6 # 1942 3817 465 147 1241 2861 758 # 3410 392 225 3613 1312 1018 2459 # 1588 1473 1993 109 1445 2324 3741 # 3780 1404 2720 3534 10 2936 2016 # 581 3130 1300 2961 757 1006 2162 # 1953 3224 3856 651 1102 3286 651 # 2213 3689 1674 1289 2898 1883 1073 # 3073 1763 4051 3364 1377 2491 189 # 751 2689 3100 507 151 1789 286 # 877 3532 382 2404 665 3035 2102 # 1136 2883 281 551 2883 4080 655 # 2989 993 3783 4027 3286 1612 1535 # 1467 4017 3716 579 2151 2193 3986 # 1061 3377 2169 1804 1449 1609 1446 # 1070 2202 39 1831 604 2772 334 # 504 3918 3986 3664 2585 2222 2204 # 2352 4017 86 1405 3397 2671 547 # 1057 3397 3049 2868 1786 2309 2039 # 602 952 3523 2127 2609 724 1278"

codec_array = load_snac_token(codec_label)
token_list = codec_array.T.tolist()
audiolist = reconscruct_snac(token_list)
audio = reconstruct_tensors(audiolist)
with torch.inference_mode():
    audio_hat = snacmodel.decode(audio)

# sf.write("/Users/liupeng/data/audio/test/snack_test.wav",  audio_hat.squeeze().cpu().numpy(), 24000)
sf.write("/lp/data/snack_test_l10_A.wav",  audio_hat.squeeze().cpu().numpy(), 24000)

print("Done")

text = "Hello! I'm Omni, a unique voice assistant designed for real-time interaction and capable of handling a wide range of language tasks, from answering questions to providing complex solutions. Unlike other assistant models, I am trained on advanced hardware, which enhances my ability to perform audio-based reasoning and provide timely, accurate responses."